{"question_comments": [" is your array sparse (mostly 0) ? Do you care how much of the variance the top 2-3 components capture -- 50 %, 90 % ? \u2013 denis  Nov 5 '12 at 11:55 ", " No its not sparse, I have it filtered for erroneous values. Yes, I am interested in finding about how many principal components are needed to explain > 75% and >90% of the variance...but not sure how. Any ideas on this? \u2013 khan  Nov 6 '12 at 8:10 ", " look at the sorted from eigh in Doug's answer -- post the top few and the sum if you like, here or a new question. And see wikipedia PCA cumulative energy \u2013 denis  Nov 6 '12 at 12:46 "], "question_title": "Principal Component Analysis (PCA) in Python", "answers": [{"answer_votes": "41", "answer_comments": [" Enrico, thanks. I am using this 3D scenario to 3D PCA plots. Thanks again. I will get in touch if some problem occurs. \u2013 khan  Nov 5 '12 at 3:32 ", " @khan the function PCA from matplot.mlab is deprecated. ( matplotlib.org/api/\u2026 ). In addition, it uses SVD, which given the size of the OPs data matrix will be an expensive computation. Using a covariance matrix (see my answer below) you can reduce the size of the matrix in the eigenvector computation by more than 100X. \u2013 doug  Nov 5 '12 at 5:59 ", " @doug: it isn't deprecated ... they just dropped it documentation. I assume. \u2013 khan  Nov 6 '12 at 7:18 ", " I'm sad, because these three lines of code do not work! \u2013 user2988577  Mar 3 '16 at 20:29 ", " I think you want to add and change the following commands @user2988577: import numpy as np and data = np.array(np.random.randint(10,size=(10,3))) . Then I would suggest following this tutorial to help you see how to plot blog.nextgenetics.net/?e=42 \u2013 amc  Oct 10 '16 at 16:07  "], "answer_body": " You can find a PCA function in the matplotlib module:  import numpy as np\nfrom matplotlib.mlab import PCA\n\ndata = np.array(np.random.randint(10,size=(10,3)))\nresults = PCA(data)  results will store the various parameters of the PCA.\nIt is from the mlab part of matplotlib, which is the compatibility layer with the MATLAB syntax  EDIT:\non the blog nextgenetics I found a wonderful demonstration of how to perform and display a PCA with the matplotlib mlab module, have fun and check that blog! ", "question_id": 13224362}, {"answer_votes": "61", "answer_comments": [" I agree to your suggestions..seems interesting and honestly, much less memory consuming approach. I have gigs of multidimensional data and I will test these techniques to see which one works the best. Thanks :-) \u2013 khan  Nov 6 '12 at 7:21 ", " How to retrieve the 1st principal component with this method? Thanks! stackoverflow.com/questions/17916837/\u2026 \u2013 Sibbs Gambling  Jul 31 '13 at 9:22 ", " @doug-- since your test doesn't run (What's m ?  Why aren't eigenvalues, eigenvectors in the PCA return defined before they are returned? etc), it's kind of hard to use this in any useful way... \u2013 mmr  Nov 26 '14 at 18:39  ", " @mmr I've posted a working example based on this answer (in a new answer) \u2013 Mark  Jan 13 '15 at 23:26 ", " @doug NP.dot(evecs.T, data.T).T , why not simplify to np.dot(data, evecs) ? \u2013 Ela782  Apr 9 '17 at 14:55 "], "answer_body": " I posted my answer even though another answer has already been accepted; the accepted answer relies on a deprecated function ; additionally, this deprecated function is based on Singular Value Decomposition (SVD), which (although perfectly valid) is the much more memory- and processor-intensive of the two general techniques for calculating PCA. This is particularly relevant here because of the size of the data array in the OP. Using covariance-based PCA, the array used in the computation flow is just 144 x 144 , rather than 26424 x 144 (the dimensions of the original data array).  Here's a simple working implementation of PCA using the linalg module from SciPy . Because this implementation first calculates the covariance matrix, and then performs all subsequent calculations on this array, it uses far less memory than SVD-based PCA.  (the linalg module in NumPy can also be used with no change in the code below aside from the import statement, which would be from numpy import linalg as LA .)  The two key steps in this PCA implementation are:   calculating the covariance matrix ; and  taking the eivenvectors & eigenvalues of this cov matrix   In the function below, the parameter dims_rescaled_data refers to the desired number of dimensions in the rescaled data matrix; this parameter has a default value of just two dimensions, but the code below isn't limited to two but it could be any value less than the column number of the original data array.   def PCA(data, dims_rescaled_data=2):\n    \"\"\"\n    returns: data transformed in 2 dims/columns + regenerated original data\n    pass in: data as 2D NumPy array\n    \"\"\"\n    import numpy as NP\n    from scipy import linalg as LA\n    m, n = data.shape\n    # mean center the data\n    data -= data.mean(axis=0)\n    # calculate the covariance matrix\n    R = NP.cov(data, rowvar=False)\n    # calculate eigenvectors & eigenvalues of the covariance matrix\n    # use 'eigh' rather than 'eig' since R is symmetric, \n    # the performance gain is substantial\n    evals, evecs = LA.eigh(R)\n    # sort eigenvalue in decreasing order\n    idx = NP.argsort(evals)[::-1]\n    evecs = evecs[:,idx]\n    # sort eigenvectors according to same index\n    evals = evals[idx]\n    # select the first n eigenvectors (n is desired dimension\n    # of rescaled data array, or dims_rescaled_data)\n    evecs = evecs[:, :dims_rescaled_data]\n    # carry out the transformation on the data using eigenvectors\n    # and return the re-scaled data, eigenvalues, and eigenvectors\n    return NP.dot(evecs.T, data.T).T, evals, evecs\n\ndef test_PCA(data, dims_rescaled_data=2):\n    '''\n    test by attempting to recover original data array from\n    the eigenvectors of its covariance matrix & comparing that\n    'recovered' array with the original data\n    '''\n    _ , _ , eigenvectors = PCA(data, dim_rescaled_data=2)\n    data_recovered = NP.dot(eigenvectors, m).T\n    data_recovered += data_recovered.mean(axis=0)\n    assert NP.allclose(data, data_recovered)\n\n\ndef plot_pca(data):\n    from matplotlib import pyplot as MPL\n    clr1 =  '#2026B2'\n    fig = MPL.figure()\n    ax1 = fig.add_subplot(111)\n    data_resc, data_orig = PCA(data)\n    ax1.plot(data_resc[:, 0], data_resc[:, 1], '.', mfc=clr1, mec=clr1)\n    MPL.show()\n\n>>> # iris, probably the most widely used reference data set in ML\n>>> df = \"~/iris.csv\"\n>>> data = NP.loadtxt(df, delimiter=',')\n>>> # remove class labels\n>>> data = data[:,:-1]\n>>> plot_pca(data)  The plot below is a visual representation of this PCA function on the iris data. As you can see, a 2D transformation cleanly separates class I from class II and class III (but not class II from class III, which in fact requires another dimension).  ", "question_id": 13224362}, {"answer_votes": "13", "answer_comments": [" Using loops defeats the purpose of numpy. You can achieve the covariance matrix much faster by simply doing matrix multiplication C = data.dot(data.T) \u2013 Nicholas Mancuso  May 6 '15 at 20:10 ", " Hmm or use numpy.cov I guess. Not sure why I included my own version. \u2013 Mark  May 21 '15 at 11:28 ", " The result of your data test and visualize seems randomlly. Can you explain the details how to visualize the data? Like how scatter(data[50:, 0], data[50:, 1] make sense? \u2013 Peter Zhu  Jun 23 '15 at 12:35 ", " @PeterZhu I'm not sure I understand your question. PCA transforms your data to new vectors that maximize variance. The scatter command shows the first two rows plotted against each other. So the projection of the data on on all other dimensions to make it 2D. \u2013 Mark  Jun 24 '15 at 19:02 ", " @Mark dot(V.T, data.T).T Why do you do this dancing, it should be equivalent to dot(data, V) ? Edit: Ah I see you probably just copied it from above. I added a comment in dough's answer. \u2013 Ela782  Apr 9 '17 at 14:54  "], "answer_body": " Another Python PCA using numpy. The same idea as @doug but that one didn't run.  from numpy import array, dot, mean, std, empty, argsort\nfrom numpy.linalg import eigh, solve\nfrom numpy.random import randn\nfrom matplotlib.pyplot import subplots, show\n\ndef cov(data):\n    \"\"\"\n    Covariance matrix\n    note: specifically for mean-centered data\n    note: numpy's `cov` uses N-1 as normalization\n    \"\"\"\n    return dot(X.T, X) / X.shape[0]\n    # N = data.shape[1]\n    # C = empty((N, N))\n    # for j in range(N):\n    #   C[j, j] = mean(data[:, j] * data[:, j])\n    #   for k in range(j + 1, N):\n    #       C[j, k] = C[k, j] = mean(data[:, j] * data[:, k])\n    # return C\n\ndef pca(data, pc_count = None):\n    \"\"\"\n    Principal component analysis using eigenvalues\n    note: this mean-centers and auto-scales the data (in-place)\n    \"\"\"\n    data -= mean(data, 0)\n    data /= std(data, 0)\n    C = cov(data)\n    E, V = eigh(C)\n    key = argsort(E)[::-1][:pc_count]\n    E, V = E[key], V[:, key]\n    U = dot(data, V)  # used to be dot(V.T, data.T).T\n    return U, E, V\n\n\"\"\" test data \"\"\"\ndata = array([randn(8) for k in range(150)])\ndata[:50, 2:4] += 5\ndata[50:, 2:5] += 5\n\n\"\"\" visualize \"\"\"\ntrans = pca(data, 3)[0]\nfig, (ax1, ax2) = subplots(1, 2)\nax1.scatter(data[:50, 0], data[:50, 1], c = 'r')\nax1.scatter(data[50:, 0], data[50:, 1], c = 'b')\nax2.scatter(trans[:50, 0], trans[:50, 1], c = 'r')\nax2.scatter(trans[50:, 0], trans[50:, 1], c = 'b')\nshow()  Which yields the same thing as the much shorter  from sklearn.decomposition import PCA\n\ndef pca2(data, pc_count = None):\n    return PCA(n_components = 4).fit_transform(data)  As I understand it, using eigenvalues (first way) is better for high-dimensional data and fewer samples, whereas using Singular value decomposition is better if you have more samples than dimensions. ", "question_id": 13224362}, {"answer_votes": "11", "answer_comments": [" +1 for the link to glowing python \u2013 EnricoGiampieri  Nov 5 '12 at 0:43 ", " thanks...that glowing python link is real help. \u2013 khan  Nov 5 '12 at 3:29 ", " I think the linked glowingpython blogpost has a number of mistakes in the code, be wary. (see the latest comments on the blog) \u2013 Ela782  Apr 9 '17 at 14:59 "], "answer_body": " This is a job for numpy .  And here's a tutorial demonstrating how pincipal component analysis can be done using numpy 's built-in modules like mean,cov,double,cumsum,dot,linalg,array,rank .  http://glowingpython.blogspot.sg/2011/07/principal-component-analysis-with-numpy.html  Notice that scipy also has a long explanation here \n- https://github.com/scikit-learn/scikit-learn/blob/babe4a5d0637ca172d47e1dfdd2f6f3c3ecb28db/scikits/learn/utils/extmath.py#L105  with the scikit-learn library having more code examples - https://github.com/scikit-learn/scikit-learn/blob/babe4a5d0637ca172d47e1dfdd2f6f3c3ecb28db/scikits/learn/utils/extmath.py#L105 ", "question_id": 13224362}, {"answer_votes": "4", "answer_comments": null, "answer_body": " UPDATE:  matplotlib.mlab.PCA is since release 2.2 (2018-03-06) indeed deprecated .  The library matplotlib.mlab.PCA (used in this answer ) is not deprecated. So for all the folks arriving here via Google, I'll post a complete working example tested with Python 2.7.  Use the following code with care as it uses a now deprecated library!  from matplotlib.mlab import PCA\nimport numpy\ndata = numpy.array( [[3,2,5], [-2,1,6], [-1,0,4], [4,3,4], [10,-5,-6]] )\npca = PCA(data)  Now in `pca.Y' is the original data matrix in terms of the principal components basis vectors. More details about the PCA object can be found here .  >>> pca.Y\narray([[ 0.67629162, -0.49384752,  0.14489202],\n   [ 1.26314784,  0.60164795,  0.02858026],\n   [ 0.64937611,  0.69057287, -0.06833576],\n   [ 0.60697227, -0.90088738, -0.11194732],\n   [-3.19578784,  0.10251408,  0.00681079]])  You can use matplotlib.pyplot to draw this data, just to convince yourself that the PCA yields \"good\" results. The names list is just used to annotate our five vectors.  import matplotlib.pyplot\nnames = [ \"A\", \"B\", \"C\", \"D\", \"E\" ]\nmatplotlib.pyplot.scatter(pca.Y[:,0], pca.Y[:,1])\nfor label, x, y in zip(names, pca.Y[:,0], pca.Y[:,1]):\n    matplotlib.pyplot.annotate( label, xy=(x, y), xytext=(-2, 2), textcoords='offset points', ha='right', va='bottom' )\nmatplotlib.pyplot.show()  Looking at our original vectors we'll see that data[0] (\"A\") and data[3] (\"D\") are rather similar as are data[1] (\"B\") and data[2] (\"C\"). This is reflected in the 2D plot of our PCA transformed data.  ", "question_id": 13224362}, {"answer_votes": "2", "answer_comments": null, "answer_body": " Here are scikit-learn options. With both methods, StandardScaler was used because PCA is effected by scale  Method 1: Have scikit-learn choose the minimum number of principal components such that at least x% (90% in example below) of the variance is retained.  from sklearn.datasets import load_iris\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\niris = load_iris()\n\n# mean-centers and auto-scales the data\nstandardizedData = StandardScaler().fit_transform(iris.data)\n\npca = PCA(.90)\n\nprincipalComponents = pca.fit_transform(X = standardizedData)\n\n# To get how many principal components was chosen\nprint(pca.n_components_)  Method 2: Choose the number of principal components (in this case, 2 was chosen)  from sklearn.datasets import load_iris\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\niris = load_iris()\n\nstandardizedData = StandardScaler().fit_transform(iris.data)\n\npca = PCA(n_components=2)\n\nprincipalComponents = pca.fit_transform(X = standardizedData)\n\n# to get how much variance was retained\nprint(pca.explained_variance_ratio_.sum())  Source: https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60 ", "question_id": 13224362}, {"answer_votes": "1", "answer_comments": null, "answer_body": " I've made a little script for comparing the different PCAs appeared as an answer here:  import numpy as np\nfrom scipy.linalg import svd\n\nshape = (26424, 144)\nrepeat = 20\npca_components = 2\n\ndata = np.array(np.random.randint(255, size=shape)).astype('float64')\n\n# data normalization\n# data.dot(data.T)\n# (U, s, Va) = svd(data, full_matrices=False)\n# data = data / s[0]\n\nfrom fbpca import diffsnorm\nfrom timeit import default_timer as timer\n\nfrom scipy.linalg import svd\nstart = timer()\nfor i in range(repeat):\n    (U, s, Va) = svd(data, full_matrices=False)\ntime = timer() - start\nerr = diffsnorm(data, U, s, Va)\nprint('svd time: %.3fms, accuracy: %E' % (time*1000/repeat, err))\n\n\nfrom matplotlib.mlab import PCA\nstart = timer()\n_pca = PCA(data)\nfor i in range(repeat):\n    U = _pca.project(data)\ntime = timer() - start\nerr = diffsnorm(data, U, _pca.fracs, _pca.Wt)\nprint('matplotlib PCA time: %.3fms, accuracy: %E' % (time*1000/repeat, err))\n\nfrom fbpca import pca\nstart = timer()\nfor i in range(repeat):\n    (U, s, Va) = pca(data, pca_components, True)\ntime = timer() - start\nerr = diffsnorm(data, U, s, Va)\nprint('facebook pca time: %.3fms, accuracy: %E' % (time*1000/repeat, err))\n\n\nfrom sklearn.decomposition import PCA\nstart = timer()\n_pca = PCA(n_components = pca_components)\n_pca.fit(data)\nfor i in range(repeat):\n    U = _pca.transform(data)\ntime = timer() - start\nerr = diffsnorm(data, U, _pca.explained_variance_, _pca.components_)\nprint('sklearn PCA time: %.3fms, accuracy: %E' % (time*1000/repeat, err))\n\nstart = timer()\nfor i in range(repeat):\n    (U, s, Va) = pca_mark(data, pca_components)\ntime = timer() - start\nerr = diffsnorm(data, U, s, Va.T)\nprint('pca by Mark time: %.3fms, accuracy: %E' % (time*1000/repeat, err))\n\nstart = timer()\nfor i in range(repeat):\n    (U, s, Va) = pca_doug(data, pca_components)\ntime = timer() - start\nerr = diffsnorm(data, U, s[:pca_components], Va.T)\nprint('pca by doug time: %.3fms, accuracy: %E' % (time*1000/repeat, err))  pca_mark is the pca in Mark's answer .  pca_doug is the pca in doug's answer .  Here is an example output (but the result differs very much on the data size and pca_components, so I'd recommend to run your own test with your own data. Also, facebook's pca is optimized for normalized data, so it will be faster and more accurate in that case):  svd time: 3212.228ms, accuracy: 1.907320E-10\nmatplotlib PCA time: 879.210ms, accuracy: 2.478853E+05\nfacebook pca time: 485.483ms, accuracy: 1.260335E+04\nsklearn PCA time: 169.832ms, accuracy: 7.469847E+07\npca by Mark time: 293.758ms, accuracy: 1.713129E+02\npca by doug time: 300.326ms, accuracy: 1.707492E+02 ", "question_id": 13224362}, {"answer_votes": "0", "answer_comments": null, "answer_body": " For the sake def plot_pca(data): will work, it is necessary to replace the lines  data_resc, data_orig = PCA(data)\nax1.plot(data_resc[:, 0], data_resc[:, 1], '.', mfc=clr1, mec=clr1)  with lines  newData, data_resc, data_orig = PCA(data)\nax1.plot(newData[:, 0], newData[:, 1], '.', mfc=clr1, mec=clr1) ", "question_id": 13224362}], "question_body": " I have a (26424 x 144) array and I want to perform PCA over it using Python. However, there is no particular place on the web that explains about how to achieve this task (There are some sites which just do PCA according to their own - there is no generalized way of doing so that I can find). Anybody with any sort of help will do great. ", "question_code": [], "question_id": 13224362}